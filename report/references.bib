
@article{acar2019survey,
  title = {A {{Survey}} on {{Homomorphic Encryption Schemes}}: {{Theory}} and {{Implementation}}},
  shorttitle = {A {{Survey}} on {{Homomorphic Encryption Schemes}}},
  author = {Acar, Abbas and Aksu, Hidayet and Uluagac, A. Selcuk and Conti, Mauro},
  date = {2019-07-31},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {51},
  number = {4},
  pages = {1--35},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3214303},
  url = {https://dl.acm.org/doi/10.1145/3214303},
  urldate = {2022-04-09},
  abstract = {Legacy encryption systems depend on sharing a key (public or private) among the peers involved in exchanging an encrypted message. However, this approach poses privacy concerns. The users or service providers with the key have exclusive rights on the data. Especially with popular cloud services, control over the privacy of the sensitive data is lost. Even when the keys are not shared, the encrypted material is shared with a third party that does not necessarily need to access the content. Moreover, untrusted servers, providers, and cloud operators can keep identifying elements of users long after users end the relationship with the services. Indeed,               Homomorphic Encryption               (HE), a special kind of encryption scheme, can address these concerns as it allows any third party to operate on the encrypted data without decrypting it in advance. Although this extremely useful feature of the HE scheme has been known for over 30 years, the first plausible and achievable               Fully Homomorphic Encryption               (FHE) scheme, which allows any computable function to perform on the encrypted data, was introduced by Craig Gentry in 2009. Even though this was a major achievement, different implementations so far demonstrated that FHE still needs to be improved significantly to be practical on every platform. Therefore, this survey focuses on HE and FHE schemes. First, we present the basics of HE and the details of the well-known Partially Homomorphic Encryption (PHE) and Somewhat Homomorphic Encryption (SWHE), which are important pillars for achieving FHE. Then, the main FHE families, which have become the base for the other follow-up FHE schemes, are presented. Furthermore, the implementations and recent improvements in Gentry-type FHE schemes are also surveyed. Finally, further research directions are discussed. This survey is intended to give a clear knowledge and foundation to researchers and practitioners interested in knowing, applying, and extending the state-of-the-art HE, PHE, SWHE, and FHE systems.},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/3YBBZ8JU/Acar et al. - 2019 - A Survey on Homomorphic Encryption Schemes Theory.pdf}
}

@unpublished{badawi2020alexnet,
  title = {Towards the {{AlexNet Moment}} for {{Homomorphic Encryption}}: {{HCNN}}, {{theFirst Homomorphic CNN}} on {{Encrypted Data}} with {{GPUs}}},
  shorttitle = {Towards the {{AlexNet Moment}} for {{Homomorphic Encryption}}},
  author = {Badawi, Ahmad Al and Chao, Jin and Lin, Jie and Mun, Chan Fook and Sim, Jun Jie and Tan, Benjamin Hong Meng and Nan, Xiao and Aung, Khin Mi Mi and Chandrasekhar, Vijay Ramaseshan},
  date = {2020-08-18},
  eprint = {1811.00778},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1811.00778},
  urldate = {2021-12-28},
  abstract = {Deep Learning as a Service (DLaaS) stands as a promising solution for cloud-based inference applications. In this setting, the cloud has a pre-learned model whereas the user has samples on which she wants to run the model. The biggest concern with DLaaS is the user privacy if the input samples are sensitive data. We provide here an efficient privacypreserving system by employing high-end technologies such as Fully Homomorphic Encryption (FHE), Convolutional Neural Networks (CNNs) and Graphics Processing Units (GPUs). FHE, with its widely-known feature of computing on encrypted data, empowers a wide range of privacy-concerned applications. This comes at high cost as it requires enormous computing power. In this paper, we show how to accelerate the performance of running CNNs on encrypted data with GPUs. We evaluated two CNNs to classify homomorphically the MNIST and CIFAR-10 datasets. Our solution achieved sufficient security level ({$>$} 80 bit) and reasonable classification accuracy (99\%) and (77.55\%) for MNIST and CIFAR-10, respectively. In terms of latency, we could classify an image in 5.16 seconds and 304.43 seconds for MNIST and CIFAR-10, respectively. Our system can also classify a batch of images ({$>$} 8,000) without extra overhead.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/home/wei2912/Zotero/storage/Y3NV8SNL/Badawi et al. - 2020 - Towards the AlexNet Moment for Homomorphic Encrypt.pdf}
}

@article{balbashardness,
  title = {The {{Hardness}} of {{LWE}} and {{Ring-LWE}}: {{A Survey}}},
  author = {Balbás, David},
  pages = {48},
  abstract = {The Learning with Errors (LWE) problem consists of distinguishing linear equations with noise from uniformly sampled values. LWE enjoys a hardness reduction from worst-case lattice problems, which are believed to be hard for classical and quantum computers. Besides, LWE allows for the construction of a large variety of cryptographic schemes, including fullyhomomorphic encryption and attribute-based cryptosystems. Unfortunately, LWE requires large key sizes and computation times. To improve efficiency, Ring-LWE replaces linear equations with noisy ring products. Nowadays, Ring-LWE and its variants are frequently used in the construction of post-quantum secure cryptosystems. In this survey, we give an overview of the hardness results for LWE and Ring-LWE, aiming to connect both problems and to provide good intuition to the reader. We present a proof of the strongest hardness result for Ring-LWE available the literature, which is a reduction from ideal lattice problems to its decision form. We start by introducing both Ring-LWE and LWE and their mathematical foundations, focusing on lattices and algebraic number theory. Then, we sketch the classical hardness proof for LWE and extend the proof techniques to the ring case. We also introduce informal discussions on parameter choices, weaknesses, related work, and open problems.},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/Z9DQN8PH/Balbás - The Hardness of LWE and Ring-LWE A Survey.pdf}
}

@article{boura2020chimera,
  title = {{{CHIMERA}}: {{Combining Ring-LWE-based Fully Homomorphic Encryption Schemes}}},
  shorttitle = {{{CHIMERA}}},
  author = {Boura, Christina and Gama, Nicolas and Georgieva, Mariya and Jetchev, Dimitar},
  date = {2020-01-01},
  journaltitle = {Journal of Mathematical Cryptology},
  volume = {14},
  number = {1},
  pages = {316--338},
  publisher = {{De Gruyter}},
  issn = {1862-2984},
  doi = {10.1515/jmc-2019-0026},
  url = {https://www.degruyter.com/document/doi/10.1515/jmc-2019-0026/html},
  urldate = {2022-01-15},
  abstract = {This paper proposes a practical hybrid solution for combining and switching between three popular Ring-LWE-based FHE schemes: TFHE, B/FV and HEAAN. This is achieved by first mapping the different plaintext spaces to a common algebraic structure and then by applying efficient switching algorithms. This approach has many practical applications. First and foremost, it becomes an integral tool for the recent standardization initiatives of homomorphic schemes and common APIs. Then, it can be used in many real-life scenarios where operations of different nature and not achievable within a single FHE scheme have to be performed and where it is important to efficiently switch from one scheme to another. Finally, as a byproduct of our analysis we introduce the notion of a FHE module structure, that generalizes the notion of the external product, but can certainly be of independent interest in future research in FHE.},
  langid = {english},
  keywords = {B/FV,floating point computation,fully homomorphic encryption,HEAAN,lattice based cryptography,Ring-LWE,TFHE},
  file = {/home/wei2912/Zotero/storage/KTUZPHU3/Boura et al. - 2020 - CHIMERA Combining Ring-LWE-based Fully Homomorphi.pdf}
}

@incollection{bourse2018fast,
  title = {Fast {{Homomorphic Evaluation}} of {{Deep Discretized Neural Networks}}},
  booktitle = {Advances in {{Cryptology}} – {{CRYPTO}} 2018},
  author = {Bourse, Florian and Minelli, Michele and Minihold, Matthias and Paillier, Pascal},
  editor = {Shacham, Hovav and Boldyreva, Alexandra},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {10993},
  pages = {483--512},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-96878-0_17},
  url = {https://link.springer.com/10.1007/978-3-319-96878-0_17},
  urldate = {2022-05-26},
  abstract = {The rise of machine learning as a service multiplies scenarios where one faces a privacy dilemma: either sensitive user data must be revealed to the entity that evaluates the cognitive model (e.g., in the Cloud), or the model itself must be revealed to the user so that the evaluation can take place locally. Fully Homomorphic Encryption (FHE) offers an elegant way to reconcile these conflicting interests in the Cloudbased scenario and also preserve non-interactivity. However, due to the inefficiency of existing FHE schemes, most applications prefer to use Somewhat Homomorphic Encryption (SHE), where the complexity of the computation to be performed has to be known in advance, and the efficiency of the scheme depends on this global complexity.},
  isbn = {978-3-319-96877-3 978-3-319-96878-0},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/TVI7XMYA/Bourse et al. - 2018 - Fast Homomorphic Evaluation of Deep Discretized Ne.pdf}
}

@article{brakerski2014leveled,
  title = {({{Leveled}}) {{Fully Homomorphic Encryption}} without {{Bootstrapping}}},
  author = {Brakerski, Zvika and Gentry, Craig and Vaikuntanathan, Vinod},
  date = {2014-07},
  journaltitle = {ACM Transactions on Computation Theory},
  shortjournal = {ACM Trans. Comput. Theory},
  volume = {6},
  number = {3},
  pages = {1--36},
  issn = {1942-3454, 1942-3462},
  doi = {10.1145/2633600},
  url = {https://dl.acm.org/doi/10.1145/2633600},
  urldate = {2022-04-09},
  abstract = {We present a novel approach to fully homomorphic encryption (FHE) that dramatically improves performance and bases security on weaker assumptions. A central conceptual contribution in our work is a new way of constructing leveled, fully homomorphic encryption schemes (capable of evaluating arbitrary polynomial-size circuits of a-priori bounded depth), without Gentry’s bootstrapping procedure.                            Specifically, we offer a choice of FHE schemes based on the learning with error (LWE) or Ring LWE (RLWE) problems that have 2               λ               security against known attacks. We construct the following.                                         (1) A leveled FHE scheme that can evaluate depth-               L               arithmetic circuits (composed of fan-in 2 gates) using               O               (               λ               .               L               3) per-gate computation, quasilinear in the security parameter. Security is based on RLWE for an approximation factor exponential in               L               . This construction does not use the bootstrapping procedure.                                         (2) A leveled FHE scheme that can evaluate depth-               L               arithmetic circuits (composed of fan-in 2 gates) using               O               (               λ               2) per-gate computation, which is independent of               L               . Security is based on RLWE for quasipolynomial factors. This construction uses bootstrapping as an optimization.                                         We obtain similar results for LWE, but with worse performance. All previous (leveled) FHE schemes required a per-gate computation of               Ω               (               λ               3.5), and all of them relied on subexponential hardness assumptions.                                         We introduce a number of further optimizations to our scheme based on the Ring LWE assumption. As an example, for circuits of large width (e.g., where a constant fraction of levels have width               Ω               (               λ               )), we can reduce the per-gate computation of the bootstrapped version to               O               (               λ               ), independent of               L               , by batching the bootstrapping operation. At the core of our construction is a new approach for managing the noise in lattice-based ciphertexts, significantly extending the techniques of Brakerski and Vaikuntanathan [2011b].},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/VFC5HAYU/Brakerski et al. - 2014 - (Leveled) Fully Homomorphic Encryption without Boo.pdf}
}

@article{brutzkuslow,
  title = {Low {{Latency Privacy Preserving Inference}}},
  author = {Brutzkus, Alon and Elisha, Oren and Gilad-Bachrach, Ran},
  pages = {10},
  abstract = {When applying machine learning to sensitive data, one has to find a balance between accuracy, information security, and computationalcomplexity. Recent studies combined Homomorphic Encryption with neural networks to make inferences while protecting against information leakage. However, these methods are limited by the width and depth of neural networks that can be used (and hence the accuracy) and exhibit high latency even for relatively simple networks. In this study we provide two solutions that address these limitations. In the first solution, we present more than 10× improvement in latency and enable inference on wider networks compared to prior attempts with the same level of security. The improved performance is achieved by novel methods to represent the data during the computation. In the second solution, we apply the method of transfer learning to provide private inference services using deep networks with latency of ∼ 0.16 seconds. We demonstrate the efficacy of our methods on several computer vision tasks.},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/MRKDA4CQ/Brutzkus et al. - Low Latency Privacy Preserving Inference.pdf}
}

@unpublished{bulat2019xnornet,
  title = {{{XNOR-Net}}++: {{Improved Binary Neural Networks}}},
  shorttitle = {{{XNOR-Net}}++},
  author = {Bulat, Adrian and Tzimiropoulos, Georgios},
  date = {2019-09-30},
  eprint = {1909.13863},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/1909.13863},
  urldate = {2022-05-10},
  abstract = {This paper proposes an improved training algorithm for binary neural networks in which both weights and activations are binary numbers. A key but fairly overlooked feature of the current state-of-the-art method of XNOR-Net [28] is the use of analytically calculated real-valued scaling factors for re-weighting the output of binary convolutions. We argue that analytic calculation of these factors is sub-optimal. Instead, in this work, we make the following contributions: (a) we propose to fuse the activation and weight scaling factors into a single one that is learned discriminatively via backpropagation. (b) More importantly, we explore several ways of constructing the shape of the scale factors while keeping the computational budget fixed. (c) We empirically measure the accuracy of our approximations and show that they are significantly more accurate than the analytically calculated one. (d) We show that our approach significantly outperforms XNOR-Net within the same computational budget when tested on the challenging task of ImageNet classification, offering up to 6\% accuracy gain.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/wei2912/Zotero/storage/3FJE6PBC/Bulat and Tzimiropoulos - 2019 - XNOR-Net++ Improved Binary Neural Networks.pdf}
}

@unpublished{bulat2020bats,
  title = {{{BATS}}: {{Binary ArchitecTure Search}}},
  shorttitle = {{{BATS}}},
  author = {Bulat, Adrian and Martinez, Brais and Tzimiropoulos, Georgios},
  date = {2020-07-23},
  eprint = {2003.01711},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2003.01711},
  urldate = {2022-05-10},
  abstract = {This paper proposes Binary ArchitecTure Search (BATS), a framework that drastically reduces the accuracy gap between binary neural networks and their real-valued counterparts by means of Neural Architecture Search (NAS). We show that directly applying NAS to the binary domain provides very poor results. To alleviate this, we describe, to our knowledge, for the first time, the 3 key ingredients for successfully applying NAS to the binary domain. Specifically, we (1) introduce and design a novel binary-oriented search space, (2) propose a new mechanism for controlling and stabilising the resulting searched topologies, (3) propose and validate a series of new search strategies for binary networks that lead to faster convergence and lower search times. Experimental results demonstrate the effectiveness of the proposed approach and the necessity of searching in the binary space directly. Moreover, (4) we set a new state-of-the-art for binary neural networks on CIFAR10, CIFAR100 and ImageNet datasets. Code will be made available.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/wei2912/Zotero/storage/TST8YLP2/Bulat et al. - 2020 - BATS Binary ArchitecTure Search.pdf}
}

@article{bulat2021highcapacity,
  title = {High-{{Capacity Expert Binary Networks}}},
  author = {Bulat, Adrian and Tzimiropoulos, Georgios and Martinez, Brais},
  date = {2021},
  pages = {19},
  abstract = {Network binarization is a promising hardware-aware direction for creating efficient deep models. Despite its memory and computational advantages, reducing the accuracy gap between binary models and their real-valued counterparts remains an unsolved challenging research problem. To this end, we make the following 3 contributions: (a) To increase model capacity, we propose Expert Binary Convolution, which, for the first time, tailors conditional computing to binary networks by learning to select one data-specific expert binary filter at a time conditioned on input features. (b) To increase representation capacity, we propose to address the inherent information bottleneck in binary networks by introducing an efficient width expansion mechanism which keeps the binary operations within the same budget. (c) To improve network design, we propose a principled binary network search mechanism that unveils a set of network topologies of favorable properties. Overall, our method improves upon prior work, with no increase in computational cost, by ∼ 6\%, reaching a groundbreaking ∼ 71\% on ImageNet classification. Code will be made available here.},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/M8TZG8W3/Bulat et al. - 2021 - HIGH-CAPACITY EXPERT BINARY NETWORKS.pdf}
}

@article{careyexplanation,
  title = {On the {{Explanation}} and {{Implementation}} of {{Three Open-Source Fully Homomorphic Encryption Libraries}}},
  author = {Carey, Alycia},
  pages = {62},
  abstract = {While fully homomorphic encryption (FHE) is a fairly new realm of cryptography, it has shown to be a promising mode of information protection as it allows arbitrary computations on encrypted data. The development of a practical FHE scheme would enable the development of secure cloud computation over sensitive data, which is a much-needed technology in today’s trend of outsourced computation and storage. The first FHE scheme was proposed by Craig Gentry in 2009, and although it was not a practical implementation, his scheme laid the groundwork for many schemes that exist today. One main focus in FHE research is the creation of a library that allows users without much knowledge of the complexities of FHE to use the technology securely. In this paper, we will present the concepts behind FHE, together with the introduction of three open-source FHE libraries, in order to bring better understanding to how the libraries function.},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/CYA6HW5S/Carey - On the Explanation and Implementation of Three Ope.pdf}
}

@inproceedings{chen2021bnn,
  title = {“{{BNN}} - {{BN}} = ?”: {{Training Binary Neural Networks}} without {{Batch Normalization}}},
  shorttitle = {“{{BNN}} - {{BN}} = ?},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Chen, Tianlong and Zhang, Zhenyu and Ouyang, Xu and Liu, Zechun and Shen, Zhiqiang and Wang, Zhangyang},
  date = {2021-06},
  pages = {4614--4624},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPRW53098.2021.00520},
  url = {https://ieeexplore.ieee.org/document/9523086/},
  urldate = {2022-05-10},
  abstract = {Batch normalization (BN) is a key facilitator and considered essential for state-of-the-art binary neural networks (BNN). However, the BN layer is costly to calculate and is typically implemented with non-binary parameters, leaving a hurdle for the efficient implementation of BNN training. It also introduces undesirable dependence between samples within each batch. Inspired by the latest advance on Batch Normalization Free (BN-Free) training [7], we extend their framework to training BNNs, and for the first time demonstrate that BNs can be completely removed from BNN training and inference regimes. By plugging in and customizing techniques including adaptive gradient clipping, scale weight standardization, and specialized bottleneck block, a BN-free BNN is capable of maintaining competitive accuracy compared to its BN-based counterpart. Extensive experiments validate the effectiveness of our proposal across diverse BNN backbones and datasets. For example, after removing BNs from the state-of-the-art ReActNets [38], it can still be trained with our proposed methodology to achieve 92.08\%, 68.34\%, and 68.0\% accuracy on CIFAR10, CIFAR-100, and ImageNet respectively, with marginal performance drop (0.23\% ∼ 0.44\% on CIFAR and 1.40\% on ImageNet). Codes and pre-trained models are available at: https://github.com/VITA-Group/BNN\_NoBN .},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  isbn = {978-1-66544-899-4},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/KB3GTM6R/Chen et al. - 2021 - “BNN - BN = ” Training Binary Neural Networks wi.pdf}
}

@incollection{chen2021efficient,
  title = {Efficient {{Homomorphic Conversion Between}} ({{Ring}}) {{LWE Ciphertexts}}},
  booktitle = {Applied {{Cryptography}} and {{Network Security}}},
  author = {Chen, Hao and Dai, Wei and Kim, Miran and Song, Yongsoo},
  editor = {Sako, Kazue and Tippenhauer, Nils Ole},
  date = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12726},
  pages = {460--479},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-78372-3_18},
  url = {https://link.springer.com/10.1007/978-3-030-78372-3_18},
  urldate = {2022-01-21},
  abstract = {In the past few years, significant progress on homomorphic encryption (HE) has been made toward both theory and practice. The most promising HE schemes are based on the hardness of the Learning With Errors (LWE) problem or its ring variant (RLWE). In this work, we present new conversion algorithms that switch between different (R)LWE-based HE schemes to take advantage of them. Specifically, we present and combine three ideas to improve the key-switching procedure between LWE ciphertexts, transformation from LWE to RLWE, as well as packing of multiple LWE ciphertexts in a single RLWE encryption. Finally, we demonstrate an application of building a secure channel between a client and a cloud server with lightweight encryption, low communication cost, and capability of homomorphic computation.},
  isbn = {978-3-030-78371-6 978-3-030-78372-3},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/ID9DSP34/Chen et al. - 2021 - Efficient Homomorphic Conversion Between (Ring) LW.pdf}
}

@incollection{cheon2015searchandcompute,
  title = {Search-and-{{Compute}} on {{Encrypted Data}}},
  booktitle = {Financial {{Cryptography}} and {{Data Security}}},
  author = {Cheon, Jung Hee and Kim, Miran and Kim, Myungsun},
  editor = {Brenner, Michael and Christin, Nicolas and Johnson, Benjamin and Rohloff, Kurt},
  date = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {8976},
  pages = {142--159},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-48051-9_11},
  url = {http://link.springer.com/10.1007/978-3-662-48051-9_11},
  urldate = {2022-04-09},
  abstract = {Private query processing on encrypted databases allows users to obtain data from encrypted databases in such a way that the user’s sensitive data will be protected from exposure. Given an encrypted database, the users typically submit queries similar to the following examples: – How many employees in an organization make over \$100,000? – What is the average age of factory workers suffering from leukemia? Answering the above questions requires one to search and then compute over the encrypted databases in sequence. In the case of privately processing queries with only one of these operations, many efficient solutions have been developed using a special-purpose encryption scheme (e.g., searchable encryption). In this paper, we are interested in efficiently processing queries that need to perform both operations on fully encrypted databases. One immediate solution is to use several special-purpose encryption schemes at the same time, but this approach is associated with a high computational cost for maintaining multiple encryption contexts. The other solution is to use a privacy homomorphism (or fully homomorphic encryption) scheme. However, no secure solutions have been developed that meet the efficiency requirements.},
  isbn = {978-3-662-48050-2 978-3-662-48051-9},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/U8C933YF/Cheon et al. - 2015 - Search-and-Compute on Encrypted Data.pdf}
}

@incollection{cheon2017homomorphic,
  title = {Homomorphic {{Encryption}} for {{Arithmetic}} of {{Approximate Numbers}}},
  booktitle = {Advances in {{Cryptology}} – {{ASIACRYPT}} 2017},
  author = {Cheon, Jung Hee and Kim, Andrey and Kim, Miran and Song, Yongsoo},
  editor = {Takagi, Tsuyoshi and Peyrin, Thomas},
  date = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {10624},
  pages = {409--437},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-70694-8_15},
  url = {http://link.springer.com/10.1007/978-3-319-70694-8_15},
  urldate = {2022-01-15},
  abstract = {We suggest a method to construct a homomorphic encryption scheme for approximate arithmetic. It supports an approximate addition and multiplication of encrypted messages, together with a new rescaling procedure for managing the magnitude of plaintext. This procedure truncates a ciphertext into a smaller modulus, which leads to rounding of plaintext. The main idea is to add a noise following significant figures which contain a main message. This noise is originally added to the plaintext for security, but considered to be a part of error occurring during approximate computations that is reduced along with plaintext by rescaling. As a result, our decryption structure outputs an approximate value of plaintext with a predetermined precision.},
  isbn = {978-3-319-70693-1 978-3-319-70694-8},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/YD3NXHYU/Cheon et al. - 2017 - Homomorphic Encryption for Arithmetic of Approxima.pdf}
}

@inproceedings{chillotti2020concrete,
  title = {{{CONCRETE}}: {{Concrete Operates oN Ciphertexts Rapidly}} by {{Extending TfhE}}},
  booktitle = {{{WAHC}} 2020–8th {{Workshop}} on {{Encrypted Computing}} \& {{Applied Homomorphic Cryptography}}},
  author = {Chillotti, Ilaria and Joye, Marc and Ligier, Damien and Orfila, Jean-Baptiste and Tap, Samuel},
  date = {2020},
  volume = {15},
  file = {/home/wei2912/Zotero/storage/E8LYPZ5B/Chillotti et al. - 2020 - CONCRETE Concrete Operates oN Ciphertexts Rapidly.pdf}
}

@article{chillotti2020tfhe,
  title = {{{TFHE}}: {{Fast Fully Homomorphic Encryption Over}} the {{Torus}}},
  shorttitle = {{{TFHE}}},
  author = {Chillotti, Ilaria and Gama, Nicolas and Georgieva, Mariya and Izabachène, Malika},
  date = {2020-01},
  journaltitle = {Journal of Cryptology},
  shortjournal = {J Cryptol},
  volume = {33},
  number = {1},
  pages = {34--91},
  issn = {0933-2790, 1432-1378},
  doi = {10.1007/s00145-019-09319-x},
  url = {http://link.springer.com/10.1007/s00145-019-09319-x},
  urldate = {2022-05-31},
  abstract = {This work describes a fast fully homomorphic encryption scheme over the torus (TFHE), that revisits, generalizes and improves the fully homomorphic encryption (FHE) based on GSW and its ring variants. The simplest FHE schemes consist in bootstrapped binary gates. In this gate bootstrapping mode, we show that the scheme FHEW of [29] can be expressed only in terms of external product between a GSW and a LWE ciphertext. As a consequence of this result and of other optimizations, we decrease the running time of their bootstrapping from 690ms to 13ms single core, using 16MB bootstrapping key instead of 1GB, and preserving the security parameter. In leveled homomorphic mode, we propose two methods to manipulate packed data, in order to decrease the ciphertext expansion and to optimize the evaluation of look-up tables and arbitrary functions in RingGSW based homomorphic schemes. We also extend the automata logic, introduced in [31], to the efficient leveled evaluation of weighted automata, and present a new homomorphic counter called TBSR, that supports all the elementary operations that occur in a multiplication. These improvements speed-up the evaluation of most arithmetic functions in a packed leveled mode, with a noise overhead that remains additive. We finally present a new circuit bootstrapping that converts LWE ciphertexts into low-noise RingGSW ciphertexts in just 137ms, which makes the leveled mode of TFHE composable, and which is fast enough to speed-up arithmetic functions, compared to the gate bootstrapping approach.},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/MR66AWAG/Chillotti et al. - 2020 - TFHE Fast Fully Homomorphic Encryption Over the T.pdf}
}

@incollection{chillotti2021improved,
  title = {Improved {{Programmable Bootstrapping}} with {{Larger Precision}} and {{Efficient Arithmetic Circuits}} for {{TFHE}}},
  booktitle = {Advances in {{Cryptology}} – {{ASIACRYPT}} 2021},
  author = {Chillotti, Ilaria and Ligier, Damien and Orfila, Jean-Baptiste and Tap, Samuel},
  editor = {Tibouchi, Mehdi and Wang, Huaxiong},
  date = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {13092},
  pages = {670--699},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-92078-4_23},
  url = {https://link.springer.com/10.1007/978-3-030-92078-4_23},
  urldate = {2022-06-21},
  abstract = {Fully Homomorphic Encryption (FHE) schemes enable to compute over encrypted data. Among them, TFHE [CGGI17] has the great advantage of offering an efficient method for bootstrapping noisy ciphertexts, i.e., reduce the noise. Indeed, homomorphic computation increases the noise in ciphertexts and might compromise the encrypted message. TFHE bootstrapping, in addition to reducing the noise, also evaluates (for free) univariate functions expressed as look-up tables. It however requires to have the most significant bit of the plaintext to be known a priori, resulting in the loss of one bit of space to store messages. Furthermore it represents a non negligible overhead in terms of computation in many use cases.},
  isbn = {978-3-030-92077-7 978-3-030-92078-4},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/5TIMFU55/Chillotti et al. - 2021 - Improved Programmable Bootstrapping with Larger Pr.pdf}
}

@incollection{chillotti2021programmable,
  title = {Programmable {{Bootstrapping Enables Efficient Homomorphic Inference}} of {{Deep Neural Networks}}},
  booktitle = {Cyber {{Security Cryptography}} and {{Machine Learning}}},
  author = {Chillotti, Ilaria and Joye, Marc and Paillier, Pascal},
  editor = {Dolev, Shlomi and Margalit, Oded and Pinkas, Benny and Schwarzmann, Alexander},
  date = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {12716},
  pages = {1--19},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-78086-9_1},
  url = {https://link.springer.com/10.1007/978-3-030-78086-9_1},
  urldate = {2022-06-14},
  abstract = {In many cases, machine learning and privacy are perceived to be at odds. Privacy concerns are especially relevant when the involved data are sensitive. This paper deals with the privacy-preserving inference of deep neural networks.},
  isbn = {978-3-030-78085-2 978-3-030-78086-9},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/Z44YFPUA/Chillotti et al. - 2021 - Programmable Bootstrapping Enables Efficient Homom.pdf}
}

@misc{courbariaux2016binarized,
  title = {Binarized {{Neural Networks}}: {{Training Deep Neural Networks}} with {{Weights}} and {{Activations Constrained}} to +1 or -1},
  shorttitle = {Binarized {{Neural Networks}}},
  author = {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  date = {2016-03-17},
  number = {arXiv:1602.02830},
  eprint = {1602.02830},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1602.02830},
  urldate = {2022-05-13},
  abstract = {We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/wei2912/Zotero/storage/4G5VSDR7/Courbariaux et al. - 2016 - Binarized Neural Networks Training Deep Neural Ne.pdf}
}

@misc{deng2021sparsitycontrol,
  title = {Sparsity-{{Control Ternary Weight Networks}}},
  author = {Deng, Xiang and Zhang, Zhongfei},
  date = {2021-10-21},
  number = {arXiv:2011.00580},
  eprint = {2011.00580},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2011.00580},
  urldate = {2022-06-16},
  abstract = {Deep neural networks (DNNs) have been widely and successfully applied to various applications, but they require large amounts of memory and computational power. This severely restricts their deployment on resource-limited devices. To address this issue, many efforts have been made on training low-bit weight DNNs. In this paper, we focus on training ternary weight \{-1, 0, +1\} networks which can avoid multiplications and dramatically reduce the memory and computation requirements. A ternary weight network can be consider as a sparser version of the binary weight counterpart by replacing some -1s or 1s in the binary weights with 0s, thus leading to more efficient inference but more memory cost. However, the existing approaches to training ternary weight networks cannot control the sparsity (i.e., percentage of 0s) of the ternary weights, which undermines the advantage of ternary weights. In this paper, we propose to our best knowledge the first sparsity-control approach (SCA) to training ternary weight networks, which is simply achieved by a weight discretization regularizer (WDR). SCA is different from all the existing regularizer-based approaches in that it can control the sparsity of the ternary weights through a controller α and does not rely on gradient estimators. We theoretically and empirically show that the sparsity of the trained ternary weights is positively related to α. SCA is extremely simple, easy-to-implement, and is shown to consistently outperform the stateof-the-art approaches significantly over several benchmark datasets and even matches the performances of the full-precision weight counterparts.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/wei2912/Zotero/storage/NRWZMELQ/Deng and Zhang - 2021 - Sparsity-Control Ternary Weight Networks.pdf}
}

@article{dowlincryptonets,
  title = {{{CryptoNets}}: {{Applying Neural Networks}} to {{Encrypted Data}} with {{High Throughput}} and {{Accuracy}}},
  author = {Dowlin, Nathan and Gilad-Bachrach, Ran and Laine, Kim and Lauter, Kristin and Naehrig, Michael and Wernsing, John},
  pages = {10},
  abstract = {Applying machine learning to a problem which involves medical, financial, or other types of sensitive data, not only requires accurate predictions but also careful attention to maintaining data privacy and security. Legal and ethical requirements may prevent the use of cloud-based machine learning solutions for such tasks. In this work, we will present a method to convert learned neural networks to CryptoNets, neural networks that can be applied to encrypted data. This allows a data owner to send their data in an encrypted form to a cloud service that hosts the network. The encryption ensures that the data remains confidential since the cloud does not have access to the keys needed to decrypt it. Nevertheless, we will show that the cloud service is capable of applying the neural network to the encrypted data to make encrypted predictions, and also return them in encrypted form. These encrypted predictions can be sent back to the owner of the secret key who can decrypt them. Therefore, the cloud service does not gain any information about the raw data nor about the prediction it made. We demonstrate CryptoNets on the MNIST optical character recognition tasks. CryptoNets achieve 99\% accuracy and can make around 59000 predictions per hour on a single PC. Therefore, they allow high throughput, accurate, and private predictions.},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/6PSLFW39/Dowlin et al. - CryptoNets Applying Neural Networks to Encrypted .pdf}
}

@article{folkertsredsec,
  title = {{{REDsec}}: {{Running Encrypted Discretized Neural Networks}} in {{Seconds}}},
  author = {Folkerts, Lars and Gouert, Charles and Tsoutsos, Nektarios Georgios},
  pages = {36},
  abstract = {Machine learning as a service (MLaaS) has risen to become a prominent technology due to the large development time, amount of data, hardware costs, and level of expertise required to develop a machine learning model. However, privacy concerns prevent the adoption of MLaaS for applications with sensitive data. A promising privacy preserving solution is to use fully homomorphic encryption (FHE) to perform the ML computations. Recent advancements have lowered computational costs by several orders of magnitude, opening doors for secure practical applications to be developed. This work looks to optimize FHE-based private machine learning inference by leveraging ternary neural networks. Such neural networks, whose weights are constrained to \{-1,0,1\}, have special properties that we exploit in this work to operate efficiently in the homomorphic domain. We introduce a general framework that takes a user-defined model as input (bring-your-own-network), performs plaintext training, and efficiently evaluates private inference leveraging FHE. We perform inference experiments with the MNIST, CIFAR10, and ImageNet datasets and achieve speeds as fast as 1.6 to 2.2 orders of magnitude slower than unencrypted single-core performance.},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/PK92HEDX/Folkerts et al. - REDsec Running Encrypted Discretized Neural Netwo.pdf}
}

@article{fu2021gatenet,
  title = {{{GATENET}}: {{BRIDGING THE GAP BETWEEN BINARIZED NEURAL NETWORK AND FHE EVALUATION}}},
  author = {Fu, Cheng and Huang, Hanxian and Chen, Xinyun and Zhao, Jishen},
  date = {2021},
  pages = {9},
  abstract = {With the advance of deep learning (DL) on sensitive classification tasks, recent works propose privacy-preserving DL evaluation methods to protect the client data privacy. However, DL models are also valuable as it is costly to collect training data. Fully homomorphic encryption (FHE) offers a promising security solution to preserve the privacy of both parties. Previous work finds that binarized neural networks (BNNs) are suitable for FHE which evaluates a function represented in logic gates. Yet, the computation costs of previous BNNs under FHE are very large. In this work, we first design evaluation circuits to enable any BNN inference under FHE. By leveraging a new BNN cost metric for FHE, called gate ops, we identify that the computation bottleneck of BNNs under FHE is the adder tree. Thus, we propose GateNet to reduce adder tree depth using group convolution which was originally designed to reduce multiplication. Also, as the non-linear functions are not the computation bottleneck, we apply a more advanced non-linear function to preserve task accuracy. Results show that GateNet can achieve 13.0×/28.5×/66.4× speedup over the state-of-the-art BNNs under FHE on MNIST/CIFAR-10/ImageNet datasets while keeping a high task accuracy (-0.9\%/-1.2\%/-4.1\%). GateNet is the first work that guarantees the privacy of both model and client data on large dataset beyond MNIST.},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/4N8TARF5/Fu et al. - 2021 - GATENET BRIDGING THE GAP BETWEEN BINARIZED NEURAL.pdf}
}

@incollection{gentry2012fully,
  title = {Fully {{Homomorphic Encryption}} with {{Polylog Overhead}}},
  booktitle = {Advances in {{Cryptology}} – {{EUROCRYPT}} 2012},
  author = {Gentry, Craig and Halevi, Shai and Smart, Nigel P.},
  editor = {Pointcheval, David and Johansson, Thomas},
  date = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {7237},
  pages = {465--482},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-29011-4_28},
  url = {http://link.springer.com/10.1007/978-3-642-29011-4_28},
  urldate = {2022-01-29},
  abstract = {We show that homomorphic evaluation of (wide enough) arithmetic circuits can be accomplished with only polylogarithmic overhead. Namely, we present a construction of fully homomorphic encryption (FHE) schemes that for security parameter λ can evaluate any width-Ω(λ) circuit with t gates in time t · polylog(λ).},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  isbn = {978-3-642-29010-7 978-3-642-29011-4},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/BY848QYF/Gentry et al. - 2012 - Fully Homomorphic Encryption with Polylog Overhead.pdf}
}

@unpublished{guo2021reducing,
  title = {Reducing the {{Teacher-Student Gap}} via {{Spherical Knowledge Disitllation}}},
  author = {Guo, Jia and Chen, Minghao and Hu, Yao and Zhu, Chen and He, Xiaofei and Cai, Deng},
  date = {2021-01-12},
  eprint = {2010.07485},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2010.07485},
  urldate = {2022-05-10},
  abstract = {Knowledge distillation aims at obtaining a compact and effective model by learning the mapping function from a much larger one. Due to the limited capacity of the student, the student would underfit the teacher. Therefore, student performance would unexpectedly drop when distilling from an oversized teacher, termed the capacity gap problem. We investigate this problem by study the gap of confidence between teacher and student. We find that the magnitude of confidence is not necessary for knowledge distillation and could harm the student performance if the student are forced to learn confidence. We propose Spherical Knowledge Distillation to eliminate this gap explicitly, which eases the underfitting problem. We find this novel knowledge representation can improve compact models with much larger teachers and is robust to temperature. We conducted experiments on both CIFAR100 and ImageNet, and achieve significant improvement. Specifically, we train ResNet18 to 73.0 accuracy, which is a substantial improvement over previous SOTA and is on par with resnet34 almost twice the student size. The implementation has been shared at https://github.com/forjiuzhou/SphericalKnowledge-Distillation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/wei2912/Zotero/storage/DZ5VCG7A/Guo et al. - 2021 - Reducing the Teacher-Student Gap via Spherical Kno.pdf}
}

@incollection{halevi2014algorithms,
  title = {Algorithms in {{HElib}}},
  booktitle = {Advances in {{Cryptology}} – {{CRYPTO}} 2014},
  author = {Halevi, Shai and Shoup, Victor},
  editor = {Garay, Juan A. and Gennaro, Rosario},
  date = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {8616},
  pages = {554--571},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-44371-2_31},
  url = {http://link.springer.com/10.1007/978-3-662-44371-2_31},
  urldate = {2022-01-29},
  isbn = {978-3-662-44370-5 978-3-662-44371-2},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/RQIWXA8S/Halevi and Shoup - 2014 - Algorithms in HElib.pdf}
}

@incollection{halevi2019improved,
  title = {An {{Improved RNS Variant}} of the {{BFV Homomorphic Encryption Scheme}}},
  booktitle = {Topics in {{Cryptology}} – {{CT-RSA}} 2019},
  author = {Halevi, Shai and Polyakov, Yuriy and Shoup, Victor},
  editor = {Matsui, Mitsuru},
  date = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {11405},
  pages = {83--105},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-12612-4_5},
  url = {http://link.springer.com/10.1007/978-3-030-12612-4_5},
  urldate = {2021-12-28},
  abstract = {We present an optimized implementation of the Fan-Vercauteren variant of Brakerski’s scale-invariant homomorphic encryption scheme. Our algorithmic improvements focus on optimizing decryption and homomorphic multiplication in the Residue Number System (RNS), using the Chinese Remainder Theorem (CRT) to represent and manipulate the large coefficients in the ciphertext polynomials. In particular, we propose efficient procedures for scaling and CRT basis extension that do not require translating the numbers to standard (positional) representation. Compared to the previously proposed RNS design due to Bajard et al. [3], our procedures are simpler and faster, and introduce a lower amount of noise. We implement our optimizations in the PALISADE library and evaluate the runtime performance for the range of multiplicative depths from 1 to 100. For example, homomorphic multiplication for a depth-20 setting can be executed in 62 ms on a modern server system, which is already practical for some outsourced-computing applications. Our algorithmic improvements can also be applied to other scale-invariant homomorphic encryption schemes, such as YASHE.},
  isbn = {978-3-030-12611-7 978-3-030-12612-4},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/MB2EIM7C/Halevi et al. - 2019 - An Improved RNS Variant of the BFV Homomorphic Enc.pdf}
}

@article{halevidesign,
  title = {Design and Implementation of {{HElib}}: A Homomorphic Encryption Library},
  author = {Halevi, Shai and Shoup, Victor},
  pages = {42},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/ARVSUJ6J/Halevi and Shoup - Design and implementation of HElib a homomorphic .pdf}
}

@inproceedings{he2015delving,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12},
  pages = {1026--1034},
  publisher = {{IEEE}},
  location = {{Santiago, Chile}},
  doi = {10.1109/ICCV.2015.123},
  url = {http://ieeexplore.ieee.org/document/7410480/},
  urldate = {2022-06-09},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [33]). To our knowledge, our result is the first1 to surpass the reported human-level performance (5.1\%, [26]) on this dataset.},
  eventtitle = {2015 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-4673-8391-2},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/63IW2X5T/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf}
}

@article{helwegenlatent,
  title = {Latent {{Weights Do Not Exist}}: {{Rethinking Binarized Neural Network Optimization}}},
  author = {Helwegen, Koen and Widdicombe, James and Geiger, Lukas and Liu, Zechun and Cheng, Kwang-Ting and Nusselder, Roeland},
  pages = {12},
  abstract = {Optimization of Binarized Neural Networks (BNNs) currently relies on real-valued latent weights to accumulate small update steps. In this paper, we argue that these latent weights cannot be treated analogously to weights in real-valued networks. Instead their main role is to provide inertia during training. We interpret current methods in terms of inertia and provide novel insights into the optimization of BNNs. We subsequently introduce the first optimizer specifically designed for BNNs, Binary Optimizer (Bop), and demonstrate its performance on CIFAR-10 and ImageNet. Together, the redefinition of latent weights as inertia and the introduction of Bop enable a better understanding of BNN optimization and open up the way for further improvements in training methodologies for BNNs. Code is available at: https://github.com/plumerai/rethinking-bnn-optimization.},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/79CPWVEK/Helwegen et al. - Latent Weights Do Not Exist Rethinking Binarized .pdf}
}

@unpublished{hinton2015distilling,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  date = {2015-03-09},
  eprint = {1503.02531},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1503.02531},
  urldate = {2022-05-10},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/wei2912/Zotero/storage/LGQEUHKV/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf}
}

@unpublished{howard2017mobilenets,
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  date = {2017-04-16},
  eprint = {1704.04861},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1704.04861},
  urldate = {2022-05-08},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depthwise separable convolutions to build light weight deep neural networks. We introduce two simple global hyperparameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/wei2912/Zotero/storage/GFAQCZXY/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf}
}

@article{joye2021guide,
  title = {Guide to {{Fully Homomorphic Encryption}} over the [{{Discretized}}] {{Torus}}},
  author = {Joye, Marc},
  date = {2021-10-14},
  pages = {39},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/JMFIDEWB/Joye - Guide to Fully Homomorphic Encryption over the [Di.pdf}
}

@book{knapp2006basic,
  title = {Basic Algebra: Along with a Companion Volume {{Advanced}} Algebra},
  shorttitle = {Basic Algebra},
  author = {Knapp, Anthony William},
  date = {2006},
  series = {Cornerstones},
  publisher = {{Birkhäuser Boston Springer e-books Imprint: Birkhäuser}},
  location = {{Boston, MA}},
  abstract = {Basic Algebra and Advanced Algebra systematically develop concepts and tools in algebra that are vital to every mathematician, whether pure or applied, aspiring or established. Together, the two books give the reader a global view of algebra and its role in mathematics as a whole. Key topics and features of Basic Algebra: *Linear algebra and group theory build on each other continually *Chapters on modern algebra treat groups, rings, fields, modules, and Galois groups, with emphasis on methods of computation throughout *Three prominent themes recur and blend together at times: the analogy between integers and polynomials in one variable over a field, the interplay between linear algebra and group theory, and the relationship between number theory and geometry *Many examples and hundreds of problems are included, along with a separate 90-page section giving hints or complete solutions for most of the problems *The exposition proceeds from the particular to the general, often providing examples well before a theory that incorporates them; includes blocks of problems that introduce additional topics and applications for further study *Applications to science and engineering (e.g., the fast Fourier transform, the theory of error-correcting codes, the use of the Jordan canonical form in solving linear systems of ordinary differential equations, and constructions of interest in mathematical physics) appear in sequences of problems Basic Algebra presents the subject matter in a forward-looking way that takes into account its historical development. It is suitable as a text in a two-semester advanced undergraduate or first-year graduate sequence in algebra, possibly supplemented by some material from Advanced Algebra at the graduate level. It requires of the reader only familiarity with matrix algebra, an understanding of the geometry and reduction of linear equations, and an acquaintance with proofs},
  isbn = {978-0-8176-4529-8},
  langid = {english}
}

@book{lanet2019innovative,
  title = {Innovative {{Security Solutions}} for {{Information Technology}} and {{Communications}}: 11th {{International Conference}}, {{SecITC}} 2018, {{Bucharest}}, {{Romania}}, {{November}} 8–9, 2018, {{Revised Selected Papers}}},
  shorttitle = {Innovative {{Security Solutions}} for {{Information Technology}} and {{Communications}}},
  editor = {Lanet, Jean-Louis and Toma, Cristian},
  date = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {11359},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-12942-2},
  url = {http://link.springer.com/10.1007/978-3-030-12942-2},
  urldate = {2022-05-10},
  isbn = {978-3-030-12941-5 978-3-030-12942-2},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/AIJYF4C6/Lanet and Toma - 2019 - Innovative Security Solutions for Information Tech.pdf}
}

@article{lin2017accurate,
  title = {Towards {{Accurate Binary Convolutional Neural Network}}},
  author = {Lin, Xiaofan and Zhao, Cong and Pan, Wei},
  date = {2017},
  pages = {9},
  abstract = {We introduce a novel scheme to train binary convolutional neural networks (CNNs) – CNNs with weights and activations constrained to \{-1,+1\} at run-time. It has been known that using binary weights and activations drastically reduce memory size and accesses, and can replace arithmetic operations with more efficient bitwise operations, leading to much faster test-time inference and lower power consumption. However, previous works on binarizing CNNs usually result in severe prediction accuracy degradation. In this paper, we address this issue with two major innovations: (1) approximating full-precision weights with the linear combination of multiple binary weight bases; (2) employing multiple binary activations to alleviate information loss. The implementation of the resulting binary CNN, denoted as ABC-Net, is shown to achieve much closer performance to its full-precision counterpart, and even reach the comparable prediction accuracy on ImageNet and forest trail datasets, given adequate binary weight bases and activations.},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/7QBBHIXR/Lin et al. - Towards Accurate Binary Convolutional Neural Netwo.pdf}
}

@incollection{liu2018bireal,
  title = {Bi-{{Real Net}}: {{Enhancing}} the {{Performance}} of 1-{{Bit CNNs}} with {{Improved Representational Capability}} and {{Advanced Training Algorithm}}},
  shorttitle = {Bi-{{Real Net}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2018},
  author = {Liu, Zechun and Wu, Baoyuan and Luo, Wenhan and Yang, Xin and Liu, Wei and Cheng, Kwang-Ting},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  date = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {11219},
  pages = {747--763},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-01267-0_44},
  url = {http://link.springer.com/10.1007/978-3-030-01267-0_44},
  urldate = {2022-06-09},
  abstract = {In this work, we study the 1-bit convolutional neural networks (CNNs), of which both the weights and activations are binary. While being efficient, the classification accuracy of the current 1-bit CNNs is much worse compared to their counterpart real-valued CNN models on the large-scale dataset, like ImageNet. To minimize the performance gap between the 1-bit and real-valued CNN models, we propose a novel model, dubbed Bi-Real net, which connects the real activations (after the 1-bit convolution and/or BatchNorm layer, before the sign function) to activations of the consecutive block, through an identity shortcut. Consequently, compared to the standard 1-bit CNN, the representational capability of the Bi-Real net is significantly enhanced and the additional cost on computation is negligible. Moreover, we develop a specific training algorithm including three technical novelties for 1bit CNNs. Firstly, we derive a tight approximation to the derivative of the non-differentiable sign function with respect to activation. Secondly, we propose a magnitude-aware gradient with respect to the weight for updating the weight parameters. Thirdly, we pre-train the real-valued CNN model with a clip function, rather than the ReLU function, to better initialize the Bi-Real net. Experiments on ImageNet show that the Bi-Real net with the proposed training algorithm achieves 56.4\% and 62.2\% top-1 accuracy with 18 layers and 34 layers, respectively. Compared to the state-of-the-arts (e.g., XNOR Net), Bi-Real net achieves up to 10\% higher top-1 accuracy with more memory saving and lower computational cost.},
  isbn = {978-3-030-01266-3 978-3-030-01267-0},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/VZ4NTPI7/Liu et al. - 2018 - Bi-Real Net Enhancing the Performance of 1-Bit CN.pdf}
}

@unpublished{liu2020reactnet,
  title = {{{ReActNet}}: {{Towards Precise Binary Neural Network}} with {{Generalized Activation Functions}}},
  shorttitle = {{{ReActNet}}},
  author = {Liu, Zechun and Shen, Zhiqiang and Savvides, Marios and Cheng, Kwang-Ting},
  date = {2020-07-12},
  eprint = {2003.03488},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  url = {http://arxiv.org/abs/2003.03488},
  urldate = {2022-05-08},
  abstract = {In this paper, we propose several ideas for enhancing a binary network to close its accuracy gap from real-valued networks without incurring any additional computational cost. We first construct a baseline network by modifying and binarizing a compact real-valued network with parameter-free shortcuts, bypassing all the intermediate convolutional layers including the downsampling layers. This baseline network strikes a good trade-off between accuracy and efficiency, achieving superior performance than most of existing binary networks at approximately half of the computational cost. Through extensive experiments and analysis, we observed that the performance of binary networks is sensitive to activation distribution variations. Based on this important observation, we propose to generalize the traditional Sign and PReLU functions, denoted as RSign and RPReLU for the respective generalized functions, to enable explicit learning of the distribution reshape and shift at near-zero extra cost. Lastly, we adopt a distributional loss to further enforce the binary network to learn similar output distributions as those of a real-valued network. We show that after incorporating all these ideas, the proposed ReActNet outperforms all the state-of-thearts by a large margin. Specifically, it outperforms Real-to-Binary Net and MeliusNet29 by 4.0\% and 3.6\% respectively for the top-1 accuracy and also reduces the gap to its real-valued counterpart to within 3.0\% top-1 accuracy on ImageNet dataset. Code and models are available at: https://github.com/liuzechun/ReActNet.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/wei2912/Zotero/storage/DT442ZGC/Liu et al. - 2020 - ReActNet Towards Precise Binary Neural Network wi.pdf}
}

@article{liuhow,
  title = {How {{Do Adam}} and {{Training Strategies Help BNNs Optimization}}?},
  author = {Liu, Zechun and Shen, Zhiqiang and Li, Shichao and Helwegen, Koen and Huang, Dong and Cheng, Kwang-Ting},
  pages = {11},
  abstract = {The best performing Binary Neural Networks (BNNs) are usually attained using Adam optimization and its multi-step training variants (Rastegari et al., 2016; Liu et al., 2020). However, to the best of our knowledge, few studies explore the fundamental reasons why Adam is superior to other optimizers like SGD for BNN optimization or provide analytical explanations that support specific training strategies. To address this, in this paper we first investigate the trajectories of gradients and weights in BNNs during the training process. We show the regularization effect of second-order momentum in Adam is crucial to revitalize the weights that are dead due to the activation saturation in BNNs. We find that Adam, through its adaptive learning rate strategy, is better equipped to handle the rugged loss surface of BNNs and reaches a better optimum with higher generalization ability. Furthermore, we inspect the intriguing role of the real-valued weights in binary networks, and reveal the effect of weight decay on the stability and sluggishness of BNN optimization. Through extensive experiments and analysis, we derive a simple training scheme, building on existing Adam-based optimization, which achieves 70.5\% top-1 accuracy on the ImageNet dataset using the same architecture as the state-of-the-art ReActNet (Liu et al., 2020) while achieving 1.1\% higher accuracy. Code and models are available at https: //github.com/liuzechun/AdamBNN.},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/5GFK6BG9/Liu et al. - How Do Adam and Training Strategies Help BNNs Opti.pdf}
}

@article{livnicomputational,
  title = {On the {{Computational Efficiency}} of {{Training Neural Networks}}},
  author = {Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  pages = {9},
  abstract = {It is well-known that neural networks are computationally hard to train. On the other hand, in practice, modern day neural networks are trained efficiently using SGD and a variety of tricks that include different activation functions (e.g. ReLU), over-specification (i.e., train networks which are larger than needed), and regularization. In this paper we revisit the computational complexity of training neural networks from a modern perspective. We provide both positive and negative results, some of them yield new provably efficient and practical algorithms for training certain types of neural networks.},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/99RCU2UN/Livni et al. - On the Computational Efficiency of Training Neural.pdf}
}

@unpublished{lou2021hemet,
  title = {{{HEMET}}: {{A Homomorphic-Encryption-Friendly Privacy-Preserving Mobile Neural Network Architecture}}},
  shorttitle = {{{HEMET}}},
  author = {Lou, Qian and Jiang, Lei},
  date = {2021-05-31},
  eprint = {2106.00038},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.00038},
  urldate = {2022-04-09},
  abstract = {Recently Homomorphic Encryption (HE) is used to implement Privacy-Preserving Neural Networks (PPNNs) that perform inferences directly on encrypted data without decryption. Prior PPNNs adopt mobile network architectures such as SqueezeNet for smaller computing overhead, but we find naïvely using mobile network architectures for a PPNN does not necessarily achieve shorter inference latency. Despite having less parameters, a mobile network architecture typically introduces more layers and increases the HE multiplicative depth of a PPNN, thereby prolonging its inference latency. In this paper, we propose a HE-friendly privacy-preserving Mobile neural nETwork architecture, HEMET. Experimental results show that, compared to state-of-the-art (SOTA) PPNNs, HEMET reduces the inference latency by 59.3\% ∼ 61.2\%, and improves the inference accuracy by 0.4\% ∼ 0.5\%.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security},
  file = {/home/wei2912/Zotero/storage/RSTRM37P/Lou and Jiang - 2021 - HEMET A Homomorphic-Encryption-Friendly Privacy-P.pdf}
}

@article{louglyph,
  title = {Glyph: {{Fast}} and {{Accurately Training Deep Neural Networks}} on {{Encrypted Data}}},
  author = {Lou, Qian and Feng, Bo and Fox, Geoffrey C and Jiang, Lei},
  pages = {10},
  abstract = {Because of the lack of expertise, to gain benefits from their data, average users have to upload their private data to cloud servers they may not trust. Due to legal or privacy constraints, most users are willing to contribute only their encrypted data, and lack interests or resources to join deep neural network (DNN) training in cloud. To train a DNN on encrypted data in a completely non-interactive way, a recent work proposes a fully homomorphic encryption (FHE)-based technique implementing all activations by Brakerski-Gentry-Vaikuntanathan (BGV)-based lookup tables. However, such inefficient lookup-table-based activations significantly prolong private training latency of DNNs. In this paper, we propose, Glyph, a FHE-based technique to fast and accurately train DNNs on encrypted data by switching between TFHE (Fast Fully Homomorphic Encryption over the Torus) and BGV cryptosystems. Glyph uses logicoperation-friendly TFHE to implement nonlinear activations, while adopts vectorialarithmetic-friendly BGV to perform multiply-accumulations (MACs). Glyph further applies transfer learning on DNN training to improve test accuracy and reduce the number of MACs between ciphertext and ciphertext in convolutional layers. Our experimental results show Glyph obtains state-of-the-art accuracy, and reduces training latency by 69\% ∼ 99\% over prior FHE-based privacy-preserving techniques on encrypted datasets.},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/73C8ELLG/Lou et al. - Glyph Fast and Accurately Training Deep Neural Ne.pdf}
}

@article{loushe,
  title = {{{SHE}}: {{A Fast}} and {{Accurate Deep Neural Network}} for {{Encrypted Data}}},
  author = {Lou, Qian and Jiang, Lei},
  pages = {9},
  abstract = {Homomorphic Encryption (HE) is one of the most promising security solutions to emerging Machine Learning as a Service (MLaaS). Leveled-HE (LHE)-enabled Convolutional Neural Networks (LHECNNs) are proposed to implement MLaaS to avoid large bootstrapping overhead. However, prior LHECNNs have to pay significant computing overhead but achieve only low inference accuracy, due to their polynomial approximation activations and poolings. Stacking many polynomial approximation activation layers in a network greatly reduces inference accuracy, since the polynomial approximation activation errors lead to a low distortion of the output distribution of the next batch normalization layer. So the polynomial approximation activations and poolings have become the obstacle to a fast and accurate LHECNN model.},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/9J87SUEU/Lou and Jiang - SHE A Fast and Accurate Deep Neural Network for E.pdf}
}

@unpublished{lu2021ffconv,
  title = {{{FFConv}}: {{Fast Factorized Neural Network Inference}} on {{Encrypted Data}}},
  shorttitle = {{{FFConv}}},
  author = {Lu, Yuxiao and Lin, Jie and Jin, Chao and Wang, Zhe and Aung, Khin Mi Mi and Li, Xiaoli},
  date = {2021-02-05},
  eprint = {2102.03494},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2102.03494},
  urldate = {2022-01-10},
  abstract = {Homomorphic Encryption (HE), allowing computations on encrypted data (ciphertext) without decrypting it first, enables secure but prohibitively slow Neural Network (HENN) inference for privacy-preserving applications in clouds. To reduce HENN inference latency, one approach is to pack multiple messages into a single ciphertext in order to reduce the number of ciphertexts and support massive parallelism of Homomorphic Multiply-Add (HMA) operations between ciphertexts. However, different ciphertext packing schemes have to be designed for different convolution layers and each of them introduces overheads that are far more expensive than HMA operations. In this paper, we propose a low-rank factorization method called FFConv to unify convolution and ciphertext packing. To our knowledge, FFConv is the first work that is capable of accelerating the overheads induced by different ciphertext packing schemes simultaneously, without incurring a significant increase in noise budget. Compared to prior art LoLa and Falcon, our method reduces the inference latency by up to 87\% and 12\%, respectively, with comparable accuracy on MNIST and CIFAR-10.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security},
  file = {/home/wei2912/Zotero/storage/3K5I2X6S/Lu et al. - 2021 - FFConv Fast Factorized Neural Network Inference o.pdf}
}

@misc{martinez2020training,
  title = {Training {{Binary Neural Networks}} with {{Real-to-Binary Convolutions}}},
  author = {Martinez, Brais and Yang, Jing and Bulat, Adrian and Tzimiropoulos, Georgios},
  date = {2020-03-25},
  number = {arXiv:2003.11535},
  eprint = {2003.11535},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2003.11535},
  urldate = {2022-06-09},
  abstract = {This paper shows how to train binary networks to within a few percent points (∼ 3 − 5\%) of the full precision counterpart. We first show how to build a strong baseline, which already achieves state-of-the-art accuracy, by combining recently proposed advances and carefully adjusting the optimization procedure. Secondly, we show that by attempting to minimize the discrepancy between the output of the binary and the corresponding real-valued convolution, additional significant accuracy gains can be obtained. We materialize this idea in two complementary ways: (1) with a loss function, during training, by matching the spatial attention maps computed at the output of the binary and real-valued convolutions, and (2) in a data-driven manner, by using the real-valued activations, available during inference prior to the binarization process, for re-scaling the activations right after the binary convolution. Finally, we show that, when putting all of our improvements together, the proposed model beats the current state of the art by more than 5\% top-1 accuracy on ImageNet and reduces the gap to its realvalued counterpart to less than 3\% and 5\% top-1 accuracy on CIFAR-100 and ImageNet respectively when using a ResNet-18 architecture. Code available at https://github.com/brais-martinez/real2binary.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/wei2912/Zotero/storage/E8TL2URL/Martinez et al. - 2020 - Training Binary Neural Networks with Real-to-Binar.pdf}
}

@article{martins2018survey,
  title = {A {{Survey}} on {{Fully Homomorphic Encryption}}: {{An Engineering Perspective}}},
  shorttitle = {A {{Survey}} on {{Fully Homomorphic Encryption}}},
  author = {Martins, Paulo and Sousa, Leonel and Mariano, Artur},
  date = {2018-11-30},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {50},
  number = {6},
  pages = {1--33},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3124441},
  url = {https://dl.acm.org/doi/10.1145/3124441},
  urldate = {2022-01-14},
  abstract = {It is unlikely that a hacker is able to compromise sensitive data that is stored in an encrypted form. However, when data is to be processed, it has to be decrypted, becoming vulnerable to attacks. Homomorphic encryption fixes this vulnerability by allowing one to compute directly on encrypted data. In this survey, both previous and current Somewhat Homomorphic Encryption (SHE) schemes are reviewed, and the more powerful and recent Fully Homomorphic Encryption (FHE) schemes are comprehensively studied. The concepts that support these schemes are presented, and their performance and security are analyzed from an engineering standpoint.},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/2EA3I3RD/Martins et al. - 2018 - A Survey on Fully Homomorphic Encryption An Engin.pdf}
}

@article{meftah2021doren,
  title = {{{DOReN}}: {{Toward Efficient Deep Convolutional Neural Networks}} with {{Fully Homomorphic Encryption}}},
  shorttitle = {{{DOReN}}},
  author = {Meftah, Souhail and Tan, Benjamin Hong Meng and Mun, Chan Fook and Aung, Khin Mi Mi and Veeravalli, Bharadwaj and Chandrasekhar, Vijay},
  date = {2021},
  journaltitle = {IEEE Transactions on Information Forensics and Security},
  shortjournal = {IEEE Trans.Inform.Forensic Secur.},
  volume = {16},
  pages = {3740--3752},
  issn = {1556-6013, 1556-6021},
  doi = {10.1109/TIFS.2021.3090959},
  url = {https://ieeexplore.ieee.org/document/9460962/},
  urldate = {2022-05-18},
  abstract = {Fully homomorphic encryption (FHE) is a powerful cryptographic primitive to secure outsourced computations against an untrusted third-party provider. With the growing demand for AI and the usefulness of machine learning as a service (MLaaS), the need for secure training and inference of artificial neural networks is rising. However, the computational complexity of existing FHE schemes has been a strong deterrent to this. Prior works suffered from accuracy degradation, lack of scalability, and ciphertext expansion issues. In this paper, we take the first step towards the problem of space-efficiency in evaluating deep neural networks through designing DOReN: a low depth, batched neuron that can simultaneously evaluate multiple quantized ReLU-activated neurons on encrypted data without approximations. Our circuit design reduced the complexity of the accumulator circuit depth from O(log m · log n) to O(log m + log n) for n bit integers. The experimental results show that the amortized processing time of our homomorphic neuron is approximately 1.26 seconds for 300 inputs and less than 0.13 seconds for 10 inputs at 80 bit security, which is a 20 fold improvement upon Lou and Jiang, NeurIPS 2019.},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/WTEJFT6V/Meftah et al. - 2021 - DOReN Toward Efficient Deep Convolutional Neural .pdf}
}

@article{miccianciolatticebased,
  title = {Lattice-Based {{Cryptography}}},
  author = {Micciancio, Daniele and Regev, Oded},
  pages = {33},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/KYP59LE7/Micciancio and Regev - Lattice-based Cryptography.pdf}
}

@article{peikertdecade,
  title = {A {{Decade}} of {{Lattice Cryptography}}},
  author = {Peikert, Chris},
  pages = {90},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/AAKQ8PCR/Peikert - A Decade of Lattice Cryptography.pdf}
}

@unpublished{rastegari2016xnornet,
  title = {{{XNOR-Net}}: {{ImageNet Classification Using Binary Convolutional Neural Networks}}},
  shorttitle = {{{XNOR-Net}}},
  author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  date = {2016-08-02},
  eprint = {1603.05279},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1603.05279},
  urldate = {2022-05-10},
  abstract = {We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-WeightNetworks, the filters are approximated with binary values resulting in 32× memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58× faster convolutional operations (in terms of number of the high precision operations) and 32× memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is the same as the full-precision AlexNet. We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than 16\% in top-1 accuracy. Our code is available at: http://allenai.org/plato/xnornet.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/wei2912/Zotero/storage/H3RI226R/Rastegari et al. - 2016 - XNOR-Net ImageNet Classification Using Binary Con.pdf}
}

@incollection{regev2010learning,
  title = {Learning with {{Errors}} over {{Rings}}},
  booktitle = {Algorithmic {{Number Theory}}},
  author = {Regev, Oded},
  editor = {Hanrot, Guillaume and Morain, François and Thomé, Emmanuel},
  date = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {6197},
  pages = {3--3},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-14518-6_3},
  url = {http://link.springer.com/10.1007/978-3-642-14518-6_3},
  urldate = {2022-01-14},
  abstract = {The “learning with errors” (LWE) problem is to distinguish random linear equations, which have been perturbed by a small amount of noise, from truly uniform ones. The problem has been shown to be as hard as worst-case lattice problems, and in recent years it has served as the foundation for a plethora of cryptographic applications. Unfortunately, these applications are rather inefficient due to an inherent quadratic overhead in the use of LWE. A main open question was whether LWE and its applications could be made truly efficient by exploiting extra algebraic structure, as was done for lattice-based hash functions (and related primitives).},
  editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  editorbtype = {redactor},
  isbn = {978-3-642-14517-9 978-3-642-14518-6},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/HXW6L79Z/Regev - 2010 - Learning with Errors over Rings.pdf}
}

@inproceedings{viand2021sok,
  title = {{{SoK}}: {{Fully Homomorphic Encryption Compilers}}},
  shorttitle = {{{SoK}}},
  booktitle = {2021 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Viand, Alexander and Jattke, Patrick and Hithnawi, Anwar},
  date = {2021-05},
  eprint = {2101.07078},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {1092--1108},
  doi = {10.1109/SP40001.2021.00068},
  url = {http://arxiv.org/abs/2101.07078},
  urldate = {2022-05-27},
  abstract = {Fully Homomorphic Encryption (FHE) allows a third party to perform arbitrary computations on encrypted data, learning neither the inputs nor the computation results. Hence, it provides resilience in situations where computations are carried out by an untrusted or potentially compromised party. This powerful concept was first conceived by Rivest et al. in the 1970s. However, it remained unrealized until Craig Gentry presented the first feasible FHE scheme in 2009.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security},
  file = {/home/wei2912/Zotero/storage/SEU6VBCA/Viand et al. - 2021 - SoK Fully Homomorphic Encryption Compilers.pdf}
}

@unpublished{xu2019accurate,
  title = {Accurate and {{Compact Convolutional Neural Networks}} with {{Trained Binarization}}},
  author = {Xu, Zhe and Cheung, Ray C. C.},
  date = {2019-09-25},
  eprint = {1909.11366},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1909.11366},
  urldate = {2022-05-10},
  abstract = {Although convolutional neural networks (CNNs) are now widely used in various computer vision applications, its huge resource demanding on parameter storage and computation makes the deployment on mobile and embedded devices difficult. Recently, binary convolutional neural networks are explored to help alleviate this issue by quantizing both weights and activations with only 1 single bit. However, there may exist a noticeable accuracy degradation when compared with full-precision models. In this paper, we propose an improved training approach towards compact binary CNNs with higher accuracy. Trainable scaling factors for both weights and activations are introduced to increase the value range. These scaling factors will be trained jointly with other parameters via backpropagation. Besides, a specific training algorithm is developed including tight approximation for derivative of discontinuous binarization function and L2 regularization acting on weight scaling factors. With these improvements, the binary CNN achieves 92.3\% accuracy on CIFAR-10 with VGG-Small network. On ImageNet, our method also obtains 46.1\% top-1 accuracy with AlexNet and 54.2\% with Resnet-18 surpassing previous works.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/wei2912/Zotero/storage/RU5UM95I/Xu and Cheung - 2019 - Accurate and Compact Convolutional Neural Networks.pdf}
}

@article{xurecu,
  title = {{{ReCU}}: {{Reviving}} the {{Dead Weights}} in {{Binary Neural Networks}}},
  author = {Xu, Zihan and Lin, Mingbao and Liu, Jianzhuang and Chen, Jie and Shao, Ling and Gao, Yue and Tian, Yonghong and Ji, Rongrong},
  pages = {11},
  abstract = {Binary neural networks (BNNs) have received increasing attention due to their superior reductions of computation and memory. Most existing works focus on either lessening the quantization error by minimizing the gap between the full-precision weights and their binarization or designing a gradient approximation to mitigate the gradient mismatch, while leaving the “dead weights” untouched. This leads to slow convergence when training BNNs. In this paper, for the first time, we explore the influence of “dead weights” which refer to a group of weights that are barely updated during the training of BNNs, and then introduce rectified clamp unit (ReCU) to revive the “dead weights” for updating. We prove that reviving the “dead weights” by ReCU can result in a smaller quantization error. Besides, we also take into account the information entropy of the weights, and then mathematically analyze why the weight standardization can benefit BNNs. We demonstrate the inherent contradiction between minimizing the quantization error and maximizing the information entropy, and then propose an adaptive exponential scheduler to identify the range of the “dead weights”. By considering the “dead weights”, our method offers not only faster BNN training, but also state-of-the-art performance on CIFAR-10 and ImageNet, compared with recent methods. Code can be available at https://github.com/z-hXu/ReCU .},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/43R6U2TI/Xu et al. - ReCU Reviving the Dead Weights in Binary Neural N.pdf}
}

@unpublished{zhou2017incremental,
  title = {Incremental {{Network Quantization}}: {{Towards Lossless CNNs}} with {{Low-Precision Weights}}},
  shorttitle = {Incremental {{Network Quantization}}},
  author = {Zhou, Aojun and Yao, Anbang and Guo, Yiwen and Xu, Lin and Chen, Yurong},
  date = {2017-08-25},
  eprint = {1702.03044},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1702.03044},
  urldate = {2022-04-09},
  abstract = {This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A wellproven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two) 1, our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code is available at https://github.com/Zhouaojun/IncrementalNetwork-Quantization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {/home/wei2912/Zotero/storage/Y3K7TZJK/Zhou et al. - 2017 - Incremental Network Quantization Towards Lossless.pdf}
}

@misc{zhou2018dorefanet,
  title = {{{DoReFa-Net}}: {{Training Low Bitwidth Convolutional Neural Networks}} with {{Low Bitwidth Gradients}}},
  shorttitle = {{{DoReFa-Net}}},
  author = {Zhou, Shuchang and Wu, Yuxin and Ni, Zekun and Zhou, Xinyu and Wen, He and Zou, Yuheng},
  date = {2018-02-01},
  number = {arXiv:1606.06160},
  eprint = {1606.06160},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1606.06160},
  urldate = {2022-06-06},
  abstract = {We propose DoReFa-Net, a method to train convolutional neural networks that have low bitwidth weights and activations using low bitwidth parameter gradients. In particular, during backward pass, parameter gradients are stochastically quantized to low bitwidth numbers before being propagated to convolutional layers. As convolutions during forward/backward passes can now operate on low bitwidth weights and activations/gradients respectively, DoReFa-Net can use bit convolution kernels to accelerate both training and inference. Moreover, as bit convolutions can be efficiently implemented on CPU, FPGA, ASIC and GPU, DoReFa-Net opens the way to accelerate training of low bitwidth neural network on these hardware. Our experiments on SVHN and ImageNet datasets prove that DoReFa-Net can achieve comparable prediction accuracy as 32-bit counterparts. For example, a DoReFa-Net derived from AlexNet that has 1-bit weights, 2-bit activations, can be trained from scratch using 6-bit gradients to get 46.1\% top-1 accuracy on ImageNet validation set. The DoReFa-Net AlexNet model is released publicly.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/wei2912/Zotero/storage/L8AWSQQJ/Zhou et al. - 2018 - DoReFa-Net Training Low Bitwidth Convolutional Ne.pdf}
}

@inproceedings{zhuang2019structured,
  title = {Structured {{Binary Neural Networks}} for {{Accurate Image Classification}} and {{Semantic Segmentation}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhuang, Bohan and Shen, Chunhua and Tan, Mingkui and Liu, Lingqiao and Reid, Ian},
  date = {2019-06},
  pages = {413--422},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00050},
  url = {https://ieeexplore.ieee.org/document/8953869/},
  urldate = {2022-06-02},
  abstract = {In this paper, we propose to train convolutional neural networks (CNNs) with both binarized weights and activations, leading to quantized models specifically for mobile devices with limited power capacity and computation resources. Previous works on quantizing CNNs seek to approximate the floating-point information using a set of discrete values, which we call value approximation, but typically assume the same architecture as the full-precision networks. However, we take a novel “structure approximation” view for quantization—it is very likely that a different architecture may be better for best performance. In particular, we propose a “network decomposition” strategy, termed Group-Net, in which we divide the network into groups. In this way, each full-precision group can be effectively reconstructed by aggregating a set of homogeneous binary branches. In addition, we learn effective connections among groups to improve the representational capability. Moreover, the proposed Group-Net shows strong generalization to other tasks. For instance, we extend Group-Net for highly accurate semantic segmentation by embedding rich context into the binary structure. Experiments on both classification and semantic segmentation tasks demonstrate the superior performance of the proposed methods over various popular architectures. In particular, we outperform the previous best binary neural networks in terms of accuracy and major computation savings.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {/home/wei2912/Zotero/storage/NIITXN5A/Zhuang et al. - 2019 - Structured Binary Neural Networks for Accurate Ima.pdf}
}

@article{zhuang2021effective,
  title = {Effective {{Training}} of {{Convolutional Neural Networks}} with {{Low-bitwidth Weights}} and {{Activations}}},
  author = {Zhuang, Bohan and Tan, Mingkui and Liu, Jing and Liu, Lingqiao and Reid, Ian and Shen, Chunhua},
  date = {2021},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--1},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2021.3088904},
  abstract = {This paper tackles the problem of training a deep convolutional neural network of both low-bitwidth weights and activations. Optimizing a low-precision network is challenging due to the non-differentiability of the quantizer, which may result in substantial accuracy loss. To address this, we propose three practical approaches, including (i) progressive quantization; (ii) stochastic precision; and (iii) joint knowledge distillation to improve the network training. First, for progressive quantization, we propose two schemes to progressively find good local minima. Specifically, we propose to first optimize a net with quantized weights and subsequently quantize activations. This is in contrast to the traditional methods which optimize them simultaneously. Furthermore, we propose a second scheme which gradually decreases the bit-width from high-precision to low-precision during training. Second, to alleviate the excessive training burden due to the multi-round training stages, we further propose a one-stage stochastic precision strategy to randomly sample and quantize sub-networks while keeping other parts in full-precision. Finally, we propose to jointly train a full-precision model alongside the low-precision one. By doing so, the full-precision model provides hints to guide the low-precision model training and significantly improves the performance of the low-precision network. Extensive experiments show the effectiveness of the proposed methods.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {image classification,knowledge distillation,Knowledge engineering,Neural networks,Numerical models,progressive quantization,Quantization (signal),Quantized neural network,stochastic precision,Stochastic processes,Task analysis,Training},
  file = {/home/wei2912/Zotero/storage/JMS3LB2H/Zhuang et al. - 2021 - Effective Training of Convolutional Neural Network.pdf;/home/wei2912/Zotero/storage/QA2N7LRV/login.html}
}
