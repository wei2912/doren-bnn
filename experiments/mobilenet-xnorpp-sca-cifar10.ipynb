{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training settings\n",
    "NUM_EPOCHS = 150\n",
    "BATCH_SIZE = 128\n",
    "MULTIPLIER = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "CUDA_DEVICE = 3\n",
    "\n",
    "torch.cuda.set_device(CUDA_DEVICE)\n",
    "device = torch.device(f\"cuda:{CUDA_DEVICE}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doren_bnn.mobilenet import MobileNet, NetType\n",
    "from torchinfo import summary\n",
    "\n",
    "NETTYPE = NetType.XNORPP_SCA\n",
    "model = MobileNet(3, num_classes=10, nettype=NETTYPE).to(device)\n",
    "\n",
    "summary(model, input_size=(BATCH_SIZE, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "criterion = CrossEntropyLoss().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=5e-6)\n",
    "# scheduler = CosineAnnealingWarmRestarts(optimizer, 25, eta_min=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "ALPHA = 0.01\n",
    "LAMBDA = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doren_bnn.utils import Dataset, Experiment\n",
    "\n",
    "EXPERIMENT_ID = f\"archived/mobilenet-xnorpp-sca-cifar10-full-{ALPHA}-{LAMBDA}\"\n",
    "print(EXPERIMENT_ID)\n",
    "experiment = Experiment(\n",
    "    EXPERIMENT_ID, Dataset.CIFAR10, BATCH_SIZE, multiplier=MULTIPLIER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if you wish to load a previous checkpoint\n",
    "experiment.load_checkpoint(model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doren_bnn.xnorpp_sca import Conv2d_XnorPP_SCA\n",
    "\n",
    "LAMB_PERIOD = 50\n",
    "hyperparams_dict = {\n",
    "    \"alpha\": lambda _: ALPHA,\n",
    "    \"lamb\": lambda epoch: 0\n",
    "    if epoch < LAMB_PERIOD\n",
    "    else LAMBDA * (10 ** -((NUM_EPOCHS - epoch) // LAMB_PERIOD)),\n",
    "}\n",
    "\n",
    "\n",
    "def regulariser(model=None, alpha: float = ALPHA, lamb: float = LAMBDA):\n",
    "    wdrs = [\n",
    "        layer.wdr(alpha)\n",
    "        for layer in model.modules()\n",
    "        if isinstance(layer, Conv2d_XnorPP_SCA)\n",
    "    ]\n",
    "    # print([\"{:.3f}\".format(float(wdr)) for wdr in wdrs])\n",
    "    return lamb * sum(wdrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.train(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    NUM_EPOCHS,\n",
    "    device=device,\n",
    "    hyperparams_dict=hyperparams_dict,\n",
    "    regulariser=regulariser,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-time Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doren_bnn.mobilenet import MobileNet, NetType\n",
    "from torchinfo import summary\n",
    "\n",
    "NETTYPE = NetType.XNORPP_SCA\n",
    "model = MobileNet(3, num_classes=10, nettype=NETTYPE, test=True).to(device)\n",
    "\n",
    "summary(model, input_size=(BATCH_SIZE, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.load_checkpoint(model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.test(model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doren_bnn.xnorpp_sca import Conv2d_XnorPP_SCA\n",
    "\n",
    "sparsity = []\n",
    "for module in model.modules():\n",
    "    if isinstance(module, Conv2d_XnorPP_SCA):\n",
    "        print(module.in_channels, module.out_channels, module.kernel_size)\n",
    "        print(module.weight.size())\n",
    "\n",
    "        tanh_weight_sq = torch.tanh(module.weight).square()\n",
    "        quant_err = (tanh_weight_sq * (1 - tanh_weight_sq)).sum().item()\n",
    "\n",
    "        total_num_sparse = 0\n",
    "        max_num_nonsparse = -1\n",
    "        for row in module.weight:\n",
    "            num_sparse = (torch.round(torch.tanh(row)) == 0).sum().item()\n",
    "            num_nonsparse = row.numel() - num_sparse\n",
    "\n",
    "            total_num_sparse += num_sparse\n",
    "            if num_nonsparse > max_num_nonsparse:\n",
    "                max_num_nonsparse = num_nonsparse\n",
    "\n",
    "        print(max_num_nonsparse)\n",
    "        print(total_num_sparse / module.weight.numel(), quant_err)\n",
    "        print(\"---\")\n",
    "\n",
    "        sparsity.append(total_num_sparse / module.weight.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ids_1 = [i for (i, _, k) in sparsity if k == 1]\n",
    "vals_1 = [val for (_, val, k) in sparsity if k == 1]\n",
    "plt.scatter(ids_1, vals_1, c=\"red\")\n",
    "\n",
    "ids_3 = [i for (i, _, k) in sparsity if k == 3]\n",
    "vals_3 = [val for (_, val, k) in sparsity if k == 3]\n",
    "plt.scatter(ids_3, vals_3, c=\"blue\")\n",
    "\n",
    "plt.xlabel(\"layer no.\")\n",
    "plt.ylabel(\"sparsity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "caf069306253663ae2c3f287e65595efbaf9af1e1857b8d76dd0fdbf819c2df8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
