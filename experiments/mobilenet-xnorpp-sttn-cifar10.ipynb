{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training settings\n",
    "NUM_EPOCHS = 150\n",
    "BATCH_SIZE = 128\n",
    "MULTIPLIER = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "CUDA_DEVICE = 0\n",
    "\n",
    "torch.cuda.set_device(CUDA_DEVICE)\n",
    "device = torch.device(f\"cuda:{CUDA_DEVICE}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doren_bnn.mobilenet import MobileNet, NetType\n",
    "from torchinfo import summary\n",
    "\n",
    "NETTYPE = NetType.XNORPP_STTN\n",
    "model = MobileNet(3, num_classes=10, nettype=NETTYPE).to(device)\n",
    "\n",
    "summary(model, input_size=(BATCH_SIZE, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "criterion = CrossEntropyLoss().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=5e-6)\n",
    "# scheduler = CosineAnnealingWarmRestarts(optimizer, 25, eta_min=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doren_bnn.utils import Dataset, Experiment\n",
    "\n",
    "EXPERIMENT_ID = f\"mobilenet-xnorpp-sttn-cifar10-{MULTIPLIER}x\"\n",
    "print(EXPERIMENT_ID)\n",
    "experiment = Experiment(\n",
    "    EXPERIMENT_ID, Dataset.CIFAR10, BATCH_SIZE, multiplier=MULTIPLIER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if you wish to load a previous checkpoint\n",
    "experiment.load_checkpoint(model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.train(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    NUM_EPOCHS,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-time Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doren_bnn.mobilenet import MobileNet, NetType\n",
    "from torchinfo import summary\n",
    "\n",
    "NETTYPE = NetType.XNORPP_STTN\n",
    "model = MobileNet(3, num_classes=10, nettype=NETTYPE).to(device)\n",
    "\n",
    "summary(model, input_size=(BATCH_SIZE, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.load_checkpoint(model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.test(model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doren_bnn.xnorpp_sttn import Conv2d_XnorPP_STTN\n",
    "\n",
    "for module in model.modules():\n",
    "    if isinstance(module, Conv2d_XnorPP_STTN):\n",
    "        print(module.in_channels, module.out_channels, module.kernel_size)\n",
    "        print(module.weight1.size())\n",
    "\n",
    "        total_num_sparse = 0\n",
    "        max_num_nonsparse = -1\n",
    "        for (row1, row2) in zip(module.weight1, module.weight2):\n",
    "            num_sparse = torch.bitwise_xor(row1.gt(0), row2.gt(0)).sum().item()\n",
    "            num_nonsparse = row1.numel() - num_sparse\n",
    "\n",
    "            total_num_sparse += num_sparse\n",
    "            if num_nonsparse > max_num_nonsparse:\n",
    "                max_num_nonsparse = num_nonsparse\n",
    "\n",
    "        print(max_num_nonsparse)\n",
    "        print(total_num_sparse / module.weight1.numel())\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fbf9bce7bb1e4fc9ecfb96977920c6a9559d86ace38026585ed867662b29060"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
