{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training settings\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "MobileNet                                     [16, 10]                  --\n",
       "├─Upsample: 1-1                               [16, 3, 224, 224]         --\n",
       "├─Sequential: 1-2                             [16, 1024, 1, 1]          --\n",
       "│    └─MobileNet_ConvBlock: 2-1               [16, 32, 112, 112]        --\n",
       "│    │    └─Sequential: 3-1                   [16, 32, 112, 112]        902\n",
       "│    └─MobileNet_ConvDsBlock: 2-2             [16, 64, 112, 112]        --\n",
       "│    │    └─Sequential: 3-2                   [16, 64, 112, 112]        2,560\n",
       "│    └─MobileNet_ConvDsBlock: 2-3             [16, 128, 56, 56]         --\n",
       "│    │    └─Sequential: 3-3                   [16, 128, 56, 56]         9,216\n",
       "│    └─MobileNet_ConvDsBlock: 2-4             [16, 128, 56, 56]         --\n",
       "│    │    └─Sequential: 3-4                   [16, 128, 56, 56]         18,304\n",
       "│    └─MobileNet_ConvDsBlock: 2-5             [16, 256, 28, 28]         --\n",
       "│    │    └─Sequential: 3-5                   [16, 256, 28, 28]         34,816\n",
       "│    └─MobileNet_ConvDsBlock: 2-6             [16, 256, 28, 28]         --\n",
       "│    │    └─Sequential: 3-6                   [16, 256, 28, 28]         69,376\n",
       "│    └─MobileNet_ConvDsBlock: 2-7             [16, 512, 14, 14]         --\n",
       "│    │    └─Sequential: 3-7                   [16, 512, 14, 14]         135,168\n",
       "│    └─MobileNet_ConvDsBlock: 2-8             [16, 512, 14, 14]         --\n",
       "│    │    └─Sequential: 3-8                   [16, 512, 14, 14]         269,824\n",
       "│    └─MobileNet_ConvDsBlock: 2-9             [16, 512, 14, 14]         --\n",
       "│    │    └─Sequential: 3-9                   [16, 512, 14, 14]         269,824\n",
       "│    └─MobileNet_ConvDsBlock: 2-10            [16, 512, 14, 14]         --\n",
       "│    │    └─Sequential: 3-10                  [16, 512, 14, 14]         269,824\n",
       "│    └─MobileNet_ConvDsBlock: 2-11            [16, 512, 14, 14]         --\n",
       "│    │    └─Sequential: 3-11                  [16, 512, 14, 14]         269,824\n",
       "│    └─MobileNet_ConvDsBlock: 2-12            [16, 512, 14, 14]         --\n",
       "│    │    └─Sequential: 3-12                  [16, 512, 14, 14]         269,824\n",
       "│    └─MobileNet_ConvDsBlock: 2-13            [16, 1024, 7, 7]          --\n",
       "│    │    └─Sequential: 3-13                  [16, 1024, 7, 7]          532,480\n",
       "│    └─MobileNet_ConvDsBlock: 2-14            [16, 1024, 7, 7]          --\n",
       "│    │    └─Sequential: 3-14                  [16, 1024, 7, 7]          1,063,936\n",
       "│    └─AdaptiveAvgPool2d: 2-15                [16, 1024, 1, 1]          --\n",
       "├─Linear: 1-3                                 [16, 10]                  10,250\n",
       "===============================================================================================\n",
       "Total params: 3,226,128\n",
       "Trainable params: 3,226,128\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 9.08\n",
       "===============================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 1303.77\n",
       "Params size (MB): 12.90\n",
       "Estimated Total Size (MB): 1316.88\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from doren_bnn.mobilenet import MobileNet, NetType\n",
    "from torchinfo import summary\n",
    "\n",
    "NETTYPE = NetType.XNORPP\n",
    "model = MobileNet(3, num_classes=10, nettype=NETTYPE).to(device)\n",
    "\n",
    "summary(model, input_size=(BATCH_SIZE, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "criterion = CrossEntropyLoss().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=5e-6)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, 25, eta_min=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "ALPHA = 0\n",
    "LAMBDA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from doren_bnn.utils import Dataset, Experiment\n",
    "\n",
    "EXPERIMENT_ID = f\"mobilenet-xnorpp\"\n",
    "experiment = Experiment(EXPERIMENT_ID, Dataset.CIFAR10, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d712bea27d46778bf4f82762682cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  2%|2         | 5/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/wei2912/Workspace/wei2912/doren-bnn/experiments/mobilenet-xnorpp.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/wei2912/Workspace/wei2912/doren-bnn/experiments/mobilenet-xnorpp.ipynb#ch0000007?line=0'>1</a>\u001b[0m experiment\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wei2912/Workspace/wei2912/doren-bnn/experiments/mobilenet-xnorpp.ipynb#ch0000007?line=1'>2</a>\u001b[0m     device,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wei2912/Workspace/wei2912/doren-bnn/experiments/mobilenet-xnorpp.ipynb#ch0000007?line=2'>3</a>\u001b[0m     model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wei2912/Workspace/wei2912/doren-bnn/experiments/mobilenet-xnorpp.ipynb#ch0000007?line=3'>4</a>\u001b[0m     criterion,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wei2912/Workspace/wei2912/doren-bnn/experiments/mobilenet-xnorpp.ipynb#ch0000007?line=4'>5</a>\u001b[0m     optimizer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wei2912/Workspace/wei2912/doren-bnn/experiments/mobilenet-xnorpp.ipynb#ch0000007?line=5'>6</a>\u001b[0m     scheduler,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wei2912/Workspace/wei2912/doren-bnn/experiments/mobilenet-xnorpp.ipynb#ch0000007?line=6'>7</a>\u001b[0m     NUM_EPOCHS,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wei2912/Workspace/wei2912/doren-bnn/experiments/mobilenet-xnorpp.ipynb#ch0000007?line=7'>8</a>\u001b[0m     resume\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/wei2912/Workspace/wei2912/doren-bnn/experiments/mobilenet-xnorpp.ipynb#ch0000007?line=8'>9</a>\u001b[0m     alpha\u001b[39m=\u001b[39;49mALPHA,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/wei2912/Workspace/wei2912/doren-bnn/experiments/mobilenet-xnorpp.ipynb#ch0000007?line=9'>10</a>\u001b[0m     lamb\u001b[39m=\u001b[39;49mLAMBDA,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/wei2912/Workspace/wei2912/doren-bnn/experiments/mobilenet-xnorpp.ipynb#ch0000007?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Workspace/wei2912/doren-bnn/doren_bnn/utils.py:106\u001b[0m, in \u001b[0;36mExperiment.train\u001b[0;34m(self, device, model, criterion, optimizer, scheduler, num_epochs, resume, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mlamb\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m lamb \u001b[39m*\u001b[39m (\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m (\u001b[39m-\u001b[39m(num_epochs \u001b[39m-\u001b[39m epoch) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m100\u001b[39m))\n\u001b[1;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mTrain/lamb\u001b[39m\u001b[39m\"\u001b[39m, kwargs[\u001b[39m\"\u001b[39m\u001b[39mlamb\u001b[39m\u001b[39m\"\u001b[39m], epoch)\n\u001b[0;32m--> 106\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch(device, model, criterion, optimizer, epoch, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    107\u001b[0m val_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_epoch(device, model, criterion, epoch, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mTrain/lr\u001b[39m\u001b[39m\"\u001b[39m, scheduler\u001b[39m.\u001b[39mget_last_lr()[\u001b[39m0\u001b[39m], epoch)\n",
      "File \u001b[0;32m~/Workspace/wei2912/doren-bnn/doren_bnn/utils.py:140\u001b[0m, in \u001b[0;36mExperiment.train_epoch\u001b[0;34m(self, device, model, criterion, optimizer, epoch, alpha, lamb)\u001b[0m\n\u001b[1;32m    137\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    138\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> 140\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    142\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[1;32m    143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mTrain/time\u001b[39m\u001b[39m\"\u001b[39m, end \u001b[39m-\u001b[39m start, epoch)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/doren-bnn-Z3Z8sIfT-py3.10/lib64/python3.10/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/doren-bnn-Z3Z8sIfT-py3.10/lib64/python3.10/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/doren-bnn-Z3Z8sIfT-py3.10/lib64/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/doren-bnn-Z3Z8sIfT-py3.10/lib64/python3.10/site-packages/torch/optim/adamw.py:161\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m             max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    159\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 161\u001b[0m     adamw(params_with_grad,\n\u001b[1;32m    162\u001b[0m           grads,\n\u001b[1;32m    163\u001b[0m           exp_avgs,\n\u001b[1;32m    164\u001b[0m           exp_avg_sqs,\n\u001b[1;32m    165\u001b[0m           max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m           state_steps,\n\u001b[1;32m    167\u001b[0m           amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    168\u001b[0m           beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    169\u001b[0m           beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    170\u001b[0m           lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m           weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    172\u001b[0m           eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    173\u001b[0m           maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m           foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    175\u001b[0m           capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/doren-bnn-Z3Z8sIfT-py3.10/lib64/python3.10/site-packages/torch/optim/adamw.py:218\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 218\u001b[0m func(params,\n\u001b[1;32m    219\u001b[0m      grads,\n\u001b[1;32m    220\u001b[0m      exp_avgs,\n\u001b[1;32m    221\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    222\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    223\u001b[0m      state_steps,\n\u001b[1;32m    224\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    225\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    226\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    227\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    228\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    229\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    230\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    231\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/doren-bnn-Z3Z8sIfT-py3.10/lib64/python3.10/site-packages/torch/optim/adamw.py:311\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    309\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    310\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[1;32m    313\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment.train(\n",
    "    device,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    NUM_EPOCHS,\n",
    "    resume=True,\n",
    "    alpha=ALPHA,\n",
    "    lamb=LAMBDA,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment.test(device, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('doren-bnn-Z3Z8sIfT-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e83aaab8b010134e55ad60062ddf3e0da69d95d3f62b8b83a253f488e46fa313"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
